# ğŸ›°ï¸ RealTime-Reddit-BigData-ETL-Pipeline-on-AWS-Glue-Redshift

This project sets up the **base architecture for a Reddit ETL pipeline** using **Apache Airflow** as the workflow orchestrator.
The goal is to extract data from Redditâ€™s API, process it, and build a scalable pipeline for further transformation and analytics.

---

## ğŸ“ Current Progress

* âœ… Airflow environment configured using **Docker + Docker Compose**
* âœ… Postgres and Redis containers running for metadata and task queue
* âœ… Airflow Webserver accessible at [http://localhost:8080](http://localhost:8080)
* âœ… DAG structure created (`reddit_dag.py`)
* âœ… Reddit API credentials securely stored using `config.conf` and `constants.py`
* âœ… `connect_reddit()` implemented to establish API connection

---

## ğŸ—ï¸ Project Structure (So Far)

```
.
â”œâ”€ dags/
â”‚  â””â”€ reddit_dag.py
â”œâ”€ etls/
â”‚  â””â”€ reddit_etl.py
â”œâ”€ pipelines/
â”‚  â””â”€ reddit_pipeline.py
â”œâ”€ utils/
â”‚  â””â”€ constants.py
â”œâ”€ config/
â”‚  â””â”€ config.conf
â”œâ”€ docs/
â”‚  â””â”€ screenshots/         # Airflow UI & proof of setup
â”œâ”€ docker-compose.yml
â”œâ”€ Dockerfile
â”œâ”€ airflow.env
â””â”€ requirements.txt
```

---

## ğŸ³ Running the Airflow Environment

```bash
docker compose build
docker compose up -d
```

Access Airflow UI at: **[http://localhost:8080](http://localhost:8080)**
Default credentials:

```
username: admin
password: admin
```

---

## ğŸ” Reddit API Setup

* Created a Reddit app in [Reddit Developer Portal](https://www.reddit.com/prefs/apps)
* Generated **Client ID** and **Secret**
* Stored credentials in `config/config.conf` (not hardcoded in code).

---

## ğŸ§  Next Steps

* Implement extraction logic with PRAW
* Add transformations and database loading
* Set up AWS components for scaling
* Automate end-to-end pipeline scheduling

---

