
# ğŸš€ Real-Time Reddit Big Data ETL Pipeline on AWS (Glue + Redshift)

A fully automated **real-time Reddit data engineering pipeline** designed to extract, transform, and analyze live Reddit posts using a modern AWS cloud-native architecture.

The project uses:

* **Apache Airflow** for orchestration
* **AWS Glue** for transformation
* **Amazon S3** for storage
* **Amazon Redshift** and **Amazon Athena** for querying
* **Power BI**, **Amazon QuickSight**, and **Tableau** for visualization.

---

## ğŸ—ï¸ 1. System Architecture

docs/screenshots/System_Architecutre-1.png

**Key Components:**

* **Reddit API**: Live data source.
* **Airflow DAG**: Automates data ingestion.
* **PostgreSQL & Docker**: Local storage during ingestion.
* **Amazon S3 (Raw & Transformed)**: Centralized cloud storage.
* **AWS Glue & Crawler**: Metadata creation and ETL.
* **Athena + Redshift**: Query and analytics.
* **Visualization**: Dashboards with BI tools.
* **IAM**: Secure access and role management.

---

## ğŸ§­ 2. Pipeline Implementation & Workflow

### ğŸª„ Step 1 â€” Data Orchestration using Airflow

![alt text](System_Architecutre-1.png)

* A DAG named `etl_reddit_pipeline` triggers scheduled runs.
* `PythonOperator` extracts Reddit posts through API.

![alt text](<03 pipeline logs.png>)

* Logs confirm successful execution of extraction and staging.

---

### ğŸ§¾ Step 2 â€” Raw Data Extraction to CSV

![alt text](<05 Data_to_CSV converted.png>)

* Data extracted is first processed into **structured CSV format**.
* Each record includes fields like `id`, `title`, `selftext`, `score`, `num_comments`, `created_utc`, etc.

---

### ğŸª£ Step 3 â€” Upload to Amazon S3 (Raw Zone)

![alt text](<06 s3.png>)

* Files are stored in the **Raw Layer** inside S3.
* Separate buckets/folders are maintained for:

  * `raw` â€” raw ingested CSVs
  * `transformed` â€” cleaned & structured Parquet files
  * `athena_output` â€” query results

---

## ğŸ§¼ 3. ETL with AWS Glue

### âš™ï¸ Step 4 â€” Data Transformation

![alt text](<08 AWS Glue.png>)

* An AWS Glue job converts CSV to Parquet with Snappy compression.

![alt text](<09 transformed data.png>)

* The output is stored in the **transformed layer** on S3.
* This improves **storage efficiency** and **query performance**.

---

### ğŸ•¸ï¸ Step 5 â€” Metadata Management with Glue Crawler

![alt text](<10 Crawler.png>)

* Glue Crawler scans transformed S3 data.
* Automatically creates a **schema** and registers it in **AWS Glue Data Catalog** under `reddit_db`.
* Enables seamless querying via Athena and Redshift Spectrum.

---

## ğŸ§ª 4. Querying Layer

### ğŸ” Step 6 â€” Amazon Athena Query

![alt text](<11 data result.png>)

* Athena reads data directly from S3 using the Glue Data Catalog.
* Example query:

  ```sql
  SELECT * 
  FROM "AwsDataCatalog"."reddit_db"."transformed_data"
  LIMIT 10;
````

* Validates the data pipeline output.

---

### ğŸ¢ Step 7 â€” Amazon Redshift Integration

![alt text](<end redshift result-1.png>)

* **Amazon Redshift Serverless** is configured.
* External tables are mapped to S3 through the Glue Data Catalog.
* Ideal for large-scale analytics and BI integration.

---

### ğŸ“Š Step 8 â€” Query & Visualization in Redshift

![alt text](<end redshift result.png>)

* Query to analyze total scores per author:

  ```sql
  SELECT author, SUM(score) AS total_score
  FROM "awsdatacatalog"."reddit_db"."transformed_data"
  GROUP BY author
  ORDER BY total_score DESC
  LIMIT 10;
  ```

![alt text](<reddit_redshift_querry_editor (1).jpeg>)

![alt text](reddit_redshift_querry_editor.jpeg)

* The result is visualized as **bar charts**, enabling quick insights.

---

## ğŸ§° 5. Tech Stack

| Layer                     | Tools / Services                     |
| ------------------------- | ------------------------------------ |
| Ingestion & Orchestration | Apache Airflow, PostgreSQL, Docker   |
| Storage                   | Amazon S3                            |
| Transformation            | AWS Glue                             |
| Metadata Catalog          | AWS Glue Data Catalog, AWS Crawler   |
| Query Layer               | Amazon Athena, Amazon Redshift       |
| Visualization             | Power BI, Amazon QuickSight, Tableau |
| Security                  | AWS Identity and Access Management   |

---

## ğŸ§­ 6. Project Folder Structure

```
ğŸ“¦ RealTime-Reddit-BigData-ETL-Pipeline-on-AWS-Glue-Redshift
â”‚
â”œâ”€â”€ ğŸ“ dags/
â”‚   â”œâ”€â”€ etl_reddit_pipeline.py          # Airflow DAG definition
â”‚   â”œâ”€â”€ operators/                      # (optional) custom operators if any
â”‚   â””â”€â”€ utils/                          # helper scripts (e.g., logging, retries)
â”‚
â”œâ”€â”€ ğŸ“ config/
â”‚   â”œâ”€â”€ credentials_template.json       # (sample structure without secrets)
â”‚   â”œâ”€â”€ airflow_variables.json          # for airflow variables if needed
â”‚   â””â”€â”€ config.yaml                     # global configs (API endpoints, S3 paths)
â”‚
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ raw/                            # local copy of raw Reddit data
â”‚   â””â”€â”€ processed/                      # local copy of processed files
â”‚
â”œâ”€â”€ ğŸ“ etls/
â”‚   â”œâ”€â”€ reddit_extraction.py            # script to extract Reddit posts
â”‚   â”œâ”€â”€ transformation.py               # cleaning / structuring data
â”‚   â””â”€â”€ s3_upload.py                    # handles uploading data to S3
â”‚
â”œâ”€â”€ ğŸ“ pipelines/
â”‚   â””â”€â”€ reddit_pipeline_runner.py       # main orchestrator (optional, CLI run)
â”‚
â”œâ”€â”€ ğŸ“ plugins/
â”‚   â””â”€â”€ __init__.py                     # for custom Airflow plugins if required
â”‚
â”œâ”€â”€ ğŸ“ utils/
â”‚   â”œâ”€â”€ helpers.py                      # reusable helper functions
â”‚   â”œâ”€â”€ logger.py                       # structured logging
â”‚   â””â”€â”€ constants.py                    # constant variables
â”‚
â”œâ”€â”€ ğŸ“ images/
â”‚   â”œâ”€â”€ System_Architecutre.png
â”œâ”€â”€ ğŸ“„ requirements.txt                 # Python dependencies (Airflow, boto3, etc.)
â”œâ”€â”€ ğŸ“„ docker-compose.yaml              # Airflow local environment setup
â”œâ”€â”€ ğŸ“„ Dockerfile                       # containerization (optional)
â”œâ”€â”€ ğŸ“„ README.md                        # full project documentation
â”œâ”€â”€ ğŸ“„ .env.example                     # sample environment variables (no secrets)
â”œâ”€â”€ ğŸ“„ .gitignore                       # to exclude sensitive/local files
â””â”€â”€ ğŸ“„ LICENSE                          # optional, for open-source repo
```

---

## âš™ï¸ 7. How to Run Locally

1. **Start Airflow**

   ```bash
   docker-compose up -d
   ```
2. **Trigger the DAG** from Airflow UI.
3. **View output** in S3, Athena, and Redshift.

---

## ğŸš€ 8. Key Features

* âœ… Real-time Reddit data ingestion
* ğŸ§¼ Automated ETL pipeline using Airflow & Glue
* ğŸª£ S3 Medallion Architecture (Raw â†’ Transformed)
* ğŸ§­ Glue Crawler for schema inference
* ğŸ“Š Athena & Redshift for analytics
* ğŸ” Secure IAM roles and policies
* ğŸ“ˆ BI-ready dashboards

---

## ğŸŒ± 9. Future Enhancements

* â© Implement streaming ingestion with Apache Kafka or Kinesis
* ğŸ§® Integrate machine learning models on transformed data
* â˜ï¸ Add CI/CD pipeline for infra deployment using Terraform
* ğŸ“Š Real-time dashboards with Power BI / QuickSight

---

## ğŸ‘¨â€ğŸ’» 10. Author

**Raghav**
ğŸ“ Data Engineer | Cloud & Big Data Enthusiast
ğŸ”— [GitHub Repository](https://github.com/ragztigadi/RealTime-Reddit-BigData-ETL-Pipeline-on-AWS-Glue-Redshift)

```